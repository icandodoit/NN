{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN으로 패션 아이템 구분하기\n",
    "\n",
    "Convolutional Neural Network (CNN) 을 이용하여 패션아이템 구분 성능을 높여보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./.data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))       # 정규화\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./.data',\n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))       # 정규화\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴넷으로 Fashion MNIST 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습 모델 정의**\n",
    "\n",
    "* `nn.Conv2d` 모듈: 첫 두 파라미터는 입력 채널 수 (in_channels)와 출력 채널 수 (out_channels)이다. Fashion MNIST 데이터셋은 흑백이미지이므로 색상채널이 1개뿐이다.\n",
    "* 첫 컨벌루션 층에서는 10개의 특징맵을 생성하고, 두 번째  컨벌루션 층에서는 10개의 특징맵을 받아 20개의 특징맵을 생성한다.\n",
    "* 각 컨벌루션 층에서 커널의 크기는 kernel_size로 지정할 수 있으며, 숫자를 하나만 지정하면 정사각형 커널로 간주한다. 즉 kernel_size=5는 kernel_size=(5,5)를 의미한다.\n",
    "* 컨볼루션 결과로 나온 출력값에는 드롭아웃을 해준다. 드롭아웃 함수 `F.dropout` 대신 `nn.Dropout2d` 모듈을 사용하여 드롭아웃 인스턴스를 만든다.\n",
    "* 컨볼루션 층과 드롭아웃을 거친 이미지는 일반 신경망을 거친다. 첫 번째 신경망 층에서는 앞 층의 출력 크기인 320을 입력 크기로 하고, 출력 크기를 50으로 한다. 두 번째 신경망 층에서는 입력 크기를 50으로 하고, 출력 크기는 분류할 클래스 개수인 10으로 설정한다.\n",
    "* $이미지: 28*28*1 -> 첫 번째 합성곱: 5*5*10 (valid padding) ->  첫 번째 합성곱 후: 24*24*10 -> 첫 번째 풀링: 2*2 -> 첫 번째 풀링 후: 12*12*10 ->  두 번째 합성곱: 5*5*20 (valid padding) ->  두 번째 합성곱 후: 8*8*20 -> 두 번째 풀링: 2*2 -> 두 번째 풀링 후: 4*4*20 (=320) \n",
    "-> (Flattened)$\n",
    "\n",
    "<br>\n",
    "\n",
    "**입력에서 출력까지 데이터가 지나갈 길 생성**\n",
    "\n",
    "* 입력 x를 받아 첫 번째 컨볼루션 층을 거치게 한 후 풀링 함수 `F.max_pool2d`를 거치게 한다. `F.max_pool2d` 함수의 두 번째 입력은 커널 크기이다. 풀링 연산은 드롭아웃과 같이 학습 파라미터가 없으므로 개인 취향에 따라 풀링 함수 `F.max_pool2d`를 사용하거나 nn.MaxPool2d 모듈을 사용해도 좋다. 컨볼루션과 최대 풀링을 통과한 x는 활성화함수 `F.relu`를 거친다.\n",
    "* 컨볼루션 층 2개를 거친 특징맵이 된 x는 2차원 구조이다. 특징맵 이후의 출력을 하는 일반 인공 신경망은 1차원 입력을 받는다. 2차원의 특징맵을 바로 입력으로 넣을 수 없으므로 1차원으로 펴주도록 한다 (Flattened). `view` 메서드에 들어가는 첫 번째 입력 -1은 '남은 차원 모두'를 의미하며, 320은 X가 가진 원소 개수를 의미한다.\n",
    "* 앞서 추출한 특징들을 입력으로 받아 분류하는 신경망 층을 구성해 보면 첫 번째 층에서는 ReLU 활성화 함수를 거치도록 하고 과적합을 방지하기 위해 드롭아웃을 사용한다.\n",
    "* 마지막 층은 10개의 출력값 (0부터 9까지의 레이블을 갖는)을 가지는 신경망이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)              \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 \n",
    "\n",
    "* `to()` 함수는 모델의 파라미터들을 지정한 곳으로 보내는 역할을 한다. 일반적으로 CPU 1개만 사용할 경우 필요는 없지만, GPU를 사용하고자 하는 경우 `to(\"cuda\")`로 지정하여 GPU로 보내야 한다. 지정하지 않을 경우 계속 CPU에 남아 있게 되며 빠른 훈련의 이점을 누리실 수 없다.\n",
    "* 최적화 알고리즘으로 파이토치에 내장되어 있는 `optim.SGD`를 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 평가를 위해서는 교차 엔트로피를 거칠 때 `reduction='sum'`을 지정해주어 미니배치의 평균 대신 합을 받아와야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 오차를 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction='sum').item()\n",
    "\n",
    "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 돌려보기\n",
    "\n",
    "자, 이제 모든 준비가 끝났다. 코드를 돌려서 실제로 학습이 되는지 확인해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306020\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.444399\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.732238\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.632071\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.702729\n",
      "[1] Test Loss: 0.2164, Accuracy: 93.78%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.327125\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.350220\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.467204\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.519017\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.177037\n",
      "[2] Test Loss: 0.1316, Accuracy: 95.96%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.507785\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.602472\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.354356\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.204434\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.406728\n",
      "[3] Test Loss: 0.1026, Accuracy: 96.75%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.480839\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.474030\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.531560\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.142797\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.292336\n",
      "[4] Test Loss: 0.0855, Accuracy: 97.31%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.289017\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.190241\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.617773\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.259058\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.235285\n",
      "[5] Test Loss: 0.0728, Accuracy: 97.59%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.125872\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.386246\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.236709\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.262351\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.261367\n",
      "[6] Test Loss: 0.0688, Accuracy: 97.70%\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.249598\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.126954\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.372188\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.095051\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.412780\n",
      "[7] Test Loss: 0.0600, Accuracy: 98.07%\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.204033\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.197406\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.418106\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.187272\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.177987\n",
      "[8] Test Loss: 0.0588, Accuracy: 98.14%\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.236906\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.175452\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.166618\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.128866\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.112770\n",
      "[9] Test Loss: 0.0533, Accuracy: 98.24%\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.162275\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.258450\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.189654\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.220309\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.132633\n",
      "[10] Test Loss: 0.0527, Accuracy: 98.28%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.121316\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.276202\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.064492\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.194665\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.117225\n",
      "[11] Test Loss: 0.0489, Accuracy: 98.39%\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.086825\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.164358\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.142643\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.203405\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.072512\n",
      "[12] Test Loss: 0.0467, Accuracy: 98.47%\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.150493\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.235240\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.095186\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.225199\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.056000\n",
      "[13] Test Loss: 0.0448, Accuracy: 98.43%\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.202164\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.190543\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.053013\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.333158\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.080143\n",
      "[14] Test Loss: 0.0440, Accuracy: 98.61%\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.245471\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.059913\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.072114\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.143778\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.156787\n",
      "[15] Test Loss: 0.0452, Accuracy: 98.48%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.101493\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.148020\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.044170\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.096893\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.098289\n",
      "[16] Test Loss: 0.0426, Accuracy: 98.59%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.178758\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.107536\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.065033\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.047223\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.269891\n",
      "[17] Test Loss: 0.0414, Accuracy: 98.59%\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.126100\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.090556\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.053208\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.134624\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.144313\n",
      "[18] Test Loss: 0.0409, Accuracy: 98.73%\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.143478\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.197768\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.121980\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.143951\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.097469\n",
      "[19] Test Loss: 0.0392, Accuracy: 98.72%\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.115093\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.062807\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.216811\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.061573\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.113698\n",
      "[20] Test Loss: 0.0383, Accuracy: 98.70%\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.077855\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.056752\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.141202\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.049429\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.121825\n",
      "[21] Test Loss: 0.0362, Accuracy: 98.81%\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.264897\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.044367\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.147757\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.059416\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.120381\n",
      "[22] Test Loss: 0.0372, Accuracy: 98.75%\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.128217\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.103957\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.022274\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.166494\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.144749\n",
      "[23] Test Loss: 0.0377, Accuracy: 98.68%\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.167201\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.320385\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.101230\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.127087\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.133166\n",
      "[24] Test Loss: 0.0362, Accuracy: 98.79%\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.040731\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.079498\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.072253\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.031417\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.243117\n",
      "[25] Test Loss: 0.0356, Accuracy: 98.90%\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.085322\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.027525\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.080615\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.061223\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.107346\n",
      "[26] Test Loss: 0.0351, Accuracy: 98.84%\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.169319\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.111023\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.041537\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.073776\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.219453\n",
      "[27] Test Loss: 0.0355, Accuracy: 98.87%\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.285716\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.023061\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.278798\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.039978\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.046090\n",
      "[28] Test Loss: 0.0355, Accuracy: 98.81%\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.060143\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.190346\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.037899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.418544\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.049143\n",
      "[29] Test Loss: 0.0346, Accuracy: 98.96%\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.257067\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.125538\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.056162\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.119025\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.089635\n",
      "[30] Test Loss: 0.0329, Accuracy: 98.89%\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.072931\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.107166\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.237537\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.179326\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.297881\n",
      "[31] Test Loss: 0.0339, Accuracy: 98.86%\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.069051\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.280302\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.033259\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.098652\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.174550\n",
      "[32] Test Loss: 0.0339, Accuracy: 98.86%\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.115770\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.037334\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.051274\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.080083\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.197825\n",
      "[33] Test Loss: 0.0325, Accuracy: 98.95%\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.035915\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.082750\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.051253\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.033139\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.098617\n",
      "[34] Test Loss: 0.0335, Accuracy: 98.91%\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.167745\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.072992\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.115430\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.049406\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.042499\n",
      "[35] Test Loss: 0.0317, Accuracy: 99.00%\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.062833\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.175018\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.101875\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.098038\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.111933\n",
      "[36] Test Loss: 0.0332, Accuracy: 98.89%\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.101636\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.153778\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.039365\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.045720\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.084955\n",
      "[37] Test Loss: 0.0318, Accuracy: 98.99%\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.096681\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.028966\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.089966\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.046624\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.060158\n",
      "[38] Test Loss: 0.0311, Accuracy: 99.01%\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.124647\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.063734\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.158952\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.038605\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.056649\n",
      "[39] Test Loss: 0.0326, Accuracy: 98.97%\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.075901\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.038379\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.090677\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.039942\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.161513\n",
      "[40] Test Loss: 0.0306, Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**수행 결과**\n",
    "\n",
    "* PyTorch 하에서 컨볼루션 두 번 (커널 10, 20개 사용), 드롭아웃 사용: 테스트 데이터에서의 정확도 98.91%\n",
    "* Pytorch 하에서 DNN + 드롭아웃: 테스트 데이터에서의 정확도 97.15%\n",
    "* Pytorch 하에서 DNN: 테스트 데이터에서의 정확도 86.7%\n",
    "\n",
    "* 비교: 텐서플로 하에서 VGG-7 사용 (컨볼루션 네 번 (커널 32, 64, 128, 256개 사용), 드롭아웃 사용): 테스트 데이터에서의 정확도 91.6%\n",
    "\n",
    "(생각해 볼 문제) 1. 정확도를 높이기 위해 적절한 모형은 무엇인가?\n",
    "                 2. 데이터셋에 따라 과도한 학습모형은 오히려 독이 된다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
