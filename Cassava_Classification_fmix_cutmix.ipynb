{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cassava_Classification_fmix_cutmix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1QNXMJgPR0_gewWAtYgHlwZPnAy1DIq07",
      "authorship_tag": "ABX9TyPPqaGxj+yXdveygMhac+Ju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icandodoit/Vision/blob/main/Cassava_Classification_fmix_cutmix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "if not exists('fmix.zip'):\n",
        "    !wget -O fmix.zip https://github.com/ecs-vlc/fmix/archive/master.zip\n",
        "    !unzip -qq fmix.zip\n",
        "    !mv FMix-master/* ./\n",
        "    !rm -r FMix-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvh3-JaTo7u5",
        "outputId": "4ca92b9c-2f80-44cc-97dc-6c4068382033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-30 12:07:57--  https://github.com/ecs-vlc/fmix/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/ecs-vlc/FMix/zip/master [following]\n",
            "--2021-12-30 12:07:57--  https://codeload.github.com/ecs-vlc/FMix/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 13.112.159.149\n",
            "Connecting to codeload.github.com (codeload.github.com)|13.112.159.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘fmix.zip’\n",
            "\n",
            "fmix.zip                [     <=>            ] 983.14K  1.01MB/s    in 0.9s    \n",
            "\n",
            "2021-12-30 12:07:59 (1.01 MB/s) - ‘fmix.zip’ saved [1006735]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import cv2\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import timm\n",
        "\n",
        "import sklearn\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import cv2\n",
        "#import pydicom\n",
        "#from efficientnet_pytorch import EfficientNet\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "metadata": {
        "id": "tF7WpB5HPMRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh9ih-IGz_AX",
        "outputId": "08052e88-3716-41c7-9129-6bdf2c4e4c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtYfDSqDxNc9",
        "outputId": "cf6822d1-058e-4870-ffa2-b51b40850c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.62 qudida-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir ('/content/fmix')"
      ],
      "metadata": {
        "id": "_80-Ns3Vxnac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "706190ae-33e0-4e85-ee1e-0e96af1f73e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e86e29fdbda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/fmix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U catalyst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81YHsf2Hslli",
        "outputId": "1bbc8e47-05b0-4c63-c7a6-6d6a3ac73cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catalyst\n",
            "  Downloading catalyst-21.12-py2.py3-none-any.whl (544 kB)\n",
            "\u001b[K     |████████████████████████████████| 544 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 84.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (4.62.3)\n",
            "Collecting hydra-slayer>=0.1.1\n",
            "  Downloading hydra_slayer-0.4.0-py3-none-any.whl (13 kB)\n",
            "Collecting tensorboardX<2.3.0>=2.1.0\n",
            "  Downloading tensorboardX-2.2-py2.py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 94.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX<2.3.0>=2.1.0->catalyst) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX<2.3.0>=2.1.0->catalyst) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->catalyst) (3.10.0.2)\n",
            "Installing collected packages: tensorboardX, PyYAML, hydra-slayer, catalyst\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 catalyst-21.12 hydra-slayer-0.4.0 tensorboardX-2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import chdir as cd\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "A1jYPoVrakDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fmix import make_low_freq_image, binarise_mask\n",
        "import timm"
      ],
      "metadata": {
        "id": "ENsOeTCcPF9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ9y7YndVWuN"
      },
      "outputs": [],
      "source": [
        "cd ('/content')\n",
        "!unzip --qq /content/drive/MyDrive/kaggle/cassava-leaf-disease-classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sXnSoPzFb05L",
        "outputId": "60c65325-fda8-4074-fbc4-ac41352a093e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e3a9783d-0791-4fd1-b0ff-dd8931629edb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000015157.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000201771.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100042118.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000723321.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000812911.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3a9783d-0791-4fd1-b0ff-dd8931629edb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3a9783d-0791-4fd1-b0ff-dd8931629edb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3a9783d-0791-4fd1-b0ff-dd8931629edb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         image_id  label\n",
              "0  1000015157.jpg      0\n",
              "1  1000201771.jpg      3\n",
              "2   100042118.jpg      1\n",
              "3  1000723321.jpg      1\n",
              "4  1000812911.jpg      3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'fold_num': 5,\n",
        "    'seed': 719,\n",
        "    'model_arch': 'tf_efficientnet_b4_ns',\n",
        "    'img_size': 512,\n",
        "    'epochs': 10,\n",
        "    'train_bs': 16,\n",
        "    'valid_bs': 32,\n",
        "    'T_0': 10,\n",
        "    'lr': 1e-4,\n",
        "    'min_lr': 1e-6,\n",
        "    'weight_decay':1e-6,\n",
        "    'num_workers': 4,\n",
        "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
        "    'verbose_step': 1,\n",
        "    'device': 'cuda:0'\n",
        "}"
      ],
      "metadata": {
        "id": "_wn0domApdp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "njPLog5PpfoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img(path):\n",
        "    im_bgr = cv2.imread(path)\n",
        "    im_rgb = im_bgr[:, :, ::-1]          \n",
        "    return im_rgb"
      ],
      "metadata": {
        "id": "QWrJ-xqJZQL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_bbox(size, lam):                   #정해진 size내에서 생길 수 있는 box를 random하게 만들어내는 함수 for cutmix\n",
        "    W = size[0]\n",
        "    H = size[1]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)               #0부터 W까지의 랜덤한 정수 한개\n",
        "    cy = np.random.randint(H)               #0부터 H까지의 랜덤한 정수 한개\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)   #np.clip(array, min, max) \n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)   #array내의 값들 중 min보다 작으면 min으로 max보다 크면 max값으로 바꿔줌\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2"
      ],
      "metadata": {
        "id": "N7uUHqfIqFDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CassavaDataset(Dataset):\n",
        "    def __init__(self, df, data_root, \n",
        "                 transforms=None, \n",
        "                 output_label=True, \n",
        "                 one_hot_label=False,\n",
        "                 do_fmix=False, \n",
        "                 fmix_params={\n",
        "                     'alpha': 1., \n",
        "                     'decay_power': 3., \n",
        "                     'shape': (CFG['img_size'], CFG['img_size']),\n",
        "                     'max_soft': True, \n",
        "                     'reformulate': False\n",
        "                 },\n",
        "                 do_cutmix=False,\n",
        "                 cutmix_params={\n",
        "                     'alpha': 1,\n",
        "                 }\n",
        "                ):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True).copy() #drop=True옵션을 주면 기존 인덱스를 버리고 재배열\n",
        "        self.transforms = transforms\n",
        "        self.data_root = data_root\n",
        "        self.do_fmix = do_fmix\n",
        "        self.fmix_params = fmix_params\n",
        "        self.do_cutmix = do_cutmix\n",
        "        self.cutmix_params = cutmix_params\n",
        "        \n",
        "        self.output_label = output_label\n",
        "        self.one_hot_label = one_hot_label\n",
        "        \n",
        "        if output_label == True:                       \n",
        "            self.labels = self.df['label'].values\n",
        "            #print(self.labels)\n",
        "            \n",
        "            if one_hot_label is True:   \n",
        "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
        "                #print(self.labels)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        # get labels\n",
        "        if self.output_label: \n",
        "            target = self.labels[index]\n",
        "          \n",
        "        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id'])) #get_img(path) return im_rgb\n",
        "\n",
        "        if self.transforms: \n",
        "            img = self.transforms(image=img)['image']\n",
        "        \n",
        "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:  \n",
        "            with torch.no_grad():\n",
        "                #lam, mask = sample_mask(**self.fmix_params)\n",
        "                \n",
        "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7) #o.6 ~ 0.7사이의 수 뽑기\n",
        "                \n",
        "                # Make mask, get mean / std\n",
        "                # package fmix 사용 : make_low_freq_image, binarise_mask\n",
        "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
        "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
        "    \n",
        "                fmix_ix = np.random.choice(self.df.index, size=1)[0]  #fmix에 사용할 이미지의 idex를 랜덤으로 선택\n",
        "                fmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[fmix_ix]['image_id']))\n",
        "\n",
        "                if self.transforms:\n",
        "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
        "\n",
        "                mask_torch = torch.from_numpy(mask)                    #numpy \n",
        "                \n",
        "                # mix image\n",
        "                img = mask_torch*img+(1.-mask_torch)*fmix_img          #fmix img 만들기\n",
        "\n",
        "                #print(mask.shape)\n",
        "\n",
        "                #assert self.output_label==True and self.one_hot_label==True\n",
        "\n",
        "                # mix target\n",
        "                rate = mask.sum()/CFG['img_size']/CFG['img_size']      #mask.sum()/(img_size * img_size)\n",
        "                target = rate*target + (1.-rate)*self.labels[fmix_ix]  #새로운 target = target과 fmix_ix의 label의 가중평균\n",
        "                #print(target, mask, img)\n",
        "                #assert False\n",
        "        \n",
        "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
        "            #print(img.sum(), img.shape)\n",
        "            with torch.no_grad():\n",
        "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
        "                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']))\n",
        "                if self.transforms:\n",
        "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
        "                    \n",
        "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox((CFG['img_size'], CFG['img_size']), lam)\n",
        "\n",
        "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]  #cmix_img의 box부분을 img에 넣기\n",
        "\n",
        "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (CFG['img_size'] * CFG['img_size']))\n",
        "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
        "                \n",
        "            #print('-', img.sum())\n",
        "            #print(target)\n",
        "            #assert False\n",
        "                            \n",
        "        # do label smoothing\n",
        "        #print(type(img), type(target))\n",
        "        if self.output_label == True:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img"
      ],
      "metadata": {
        "id": "sXUPQ3slgDk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E12Q50oyTH3h",
        "outputId": "0cde192a-da2d-4e25-fac8-dfb78a210401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python==4.1.1.26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7FVEzF3QS7n",
        "outputId": "3036cca0-242a-4612-80cc-7722d201ab23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python==4.1.1.26\n",
            "  Downloading opencv_python-4.1.1.26-cp37-cp37m-manylinux1_x86_64.whl (28.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.7 MB 95.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.1.1.26) (1.19.5)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.5.5.62\n",
            "    Uninstalling opencv-python-4.5.5.62:\n",
            "      Successfully uninstalled opencv-python-4.5.5.62\n",
            "Successfully installed opencv-python-4.1.1.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "wGI0iPYFQl0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import cv2\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import timm\n",
        "\n",
        "import sklearn\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import cv2\n",
        "import pydicom\n",
        "#from efficientnet_pytorch import EfficientNet\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "Z6yyAbSXRKY9",
        "outputId": "56d92fc3-9231-42e3-cec5-c97101dbabd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-86d58ddf15c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpreviously_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmodule_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mcv_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodule_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Break out of outer loop when breaking out of inner loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'cv2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
        ")\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_train_transforms():\n",
        "    return Compose([\n",
        "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),  #정사각형 박스 크기로 random resize\n",
        "            Transpose(p=0.5),                                     #가로 세로 방향 바꾸기\n",
        "            HorizontalFlip(p=0.5),                                #좌우반전\n",
        "            VerticalFlip(p=0.5),                                  #상하반전\n",
        "            ShiftScaleRotate(p=0.5),                              #자유롭게 각도 변경\n",
        "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),  #색상 채도 명도 변경\n",
        "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),  #밝기 변경\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0), #정규화\n",
        "            CoarseDropout(p=0.5),  #여러개(min max 값 사이) 작은 박스를 cutout\n",
        "            Cutout(p=0.5),         #정해진 개수만큼 작은 박스 구멍 뚫기 CoarseDropout과 개수가 정해져있는지 아닌지가 구별됨.\n",
        "            ToTensorV2(p=1.0),     #tensor로 옮겨주기\n",
        "        ], p=1.)\n",
        "  \n",
        "        \n",
        "def get_valid_transforms():\n",
        "    return Compose([\n",
        "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),   #정 가운데를 기준으로 crop\n",
        "            Resize(CFG['img_size'], CFG['img_size']),             #왜 또 resize 하는지는 잘 모르겠음\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),   #정규화\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.) "
      ],
      "metadata": {
        "id": "3SX8Y2G6paou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CassvaImgClassifier(nn.Module):           # tf_efficientnet_b4_ns\n",
        "    def __init__(self, model_arch, n_class, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
        "        n_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(n_features, n_class)\n",
        "        '''\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n",
        "            nn.Linear(n_features, n_class, bias=True)\n",
        "        )\n",
        "        '''\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ydfOXSwerB1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(df, trn_idx, val_idx, data_root='/content/train_images/'):\n",
        "    \n",
        "    from catalyst.data.sampler import BalanceClassSampler\n",
        "    \n",
        "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
        "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
        "        \n",
        "    train_ds = CassavaDataset(train_, data_root, transforms=get_train_transforms(), output_label=True, one_hot_label= False, do_fmix=True, do_cutmix=True)\n",
        "    valid_ds = CassavaDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CFG['train_bs'],\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        shuffle=True,        \n",
        "        num_workers=CFG['num_workers'],\n",
        "        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        valid_ds, \n",
        "        batch_size=CFG['valid_bs'],\n",
        "        num_workers=CFG['num_workers'],\n",
        "        shuffle=False,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "yfx1cpSjzo_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
        "    model.train()          #모델을 학습상태로 저장\n",
        "\n",
        "    t = time.time()        \n",
        "    running_loss = None\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, (imgs, image_labels) in pbar:              #배치 단위로 저장된 train_loader의 순서대로 학습\n",
        "        imgs = imgs.to(device).float()                   #장비에 할당\n",
        "        image_labels = image_labels.to(device).long()    #장비에 할당\n",
        "\n",
        "        #print(image_labels.shape, exam_label.shape)\n",
        "        with autocast():                                 \n",
        "            image_preds = model(imgs)   #output = model(input)         \n",
        "            #print(image_preds.shape, exam_pred.shape)\n",
        "\n",
        "            loss = loss_fn(image_preds, image_labels)\n",
        "            \n",
        "            scaler.scale(loss).backward()              #loss 값을 계산한 결과를 바탕으로 Back Propagation을 통해 계산된 Gradient 값을 \n",
        "                                                       #각 파라미터에 할당함\n",
        "\n",
        "            if running_loss is None:                  #첫번째 학습이라서 running_loss가 Nonde일 경우에\n",
        "                running_loss = loss.item()\n",
        "            else:\n",
        "                running_loss = running_loss * .99 + loss.item() * .01               \n",
        "\n",
        "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
        "\n",
        "                scaler.step(optimizer)                #epoch마다 각 파라미터에 할당된 gradient 값을 이용해 파라미터 값을 업데이트함.\n",
        "                scaler.update()                       \n",
        "                optimizer.zero_grad()                 #optimizer내의 gradient 초기화\n",
        "                \n",
        "                if scheduler is not None and schd_batch_update:\n",
        "                    scheduler.step()\n",
        "\n",
        "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
        "                \n",
        "                pbar.set_description(description)\n",
        "                \n",
        "    if scheduler is not None and not schd_batch_update:\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "ZMinryZ5wJwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
        "    model.eval()                                 #모델을 평가상태로\n",
        "\n",
        "    t = time.time()\n",
        "    loss_sum = 0                                           \n",
        "    sample_num = 0                             \n",
        "    image_preds_all = []\n",
        "    image_targets_all = []\n",
        "    \n",
        "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
        "    for step, (imgs, image_labels) in pbar:\n",
        "        imgs = imgs.to(device).float()\n",
        "        image_labels = image_labels.to(device).long()\n",
        "        \n",
        "        image_preds = model(imgs)   #output = model(input)             #output은 class 값으로 안나오고 확률로 나옴\n",
        "        #print(image_preds.shape, exam_pred.shape)\n",
        "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]   #class 확률 중 가장 큰 것이 preds\n",
        "        image_targets_all += [image_labels.detach().cpu().numpy()]                 #정답 클래스\n",
        "        \n",
        "        loss = loss_fn(image_preds, image_labels)                    \n",
        "        \n",
        "        loss_sum += loss.item()*image_labels.shape[0]   #배치안에 이미지 수를 반영해서 (곱한) loss 더 하기\n",
        "        sample_num += image_labels.shape[0]             #배치말고 그냥 이미지 sample의 개수\n",
        "\n",
        "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
        "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}' #계산된 loss의 값을 minibatch개수만큼나눠 평균loss로계산\n",
        "            pbar.set_description(description)\n",
        "    \n",
        "    image_preds_all = np.concatenate(image_preds_all)\n",
        "    image_targets_all = np.concatenate(image_targets_all)\n",
        "    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))   #testdata가 얼마나 맞췄는지\n",
        "    \n",
        "    if scheduler is not None:\n",
        "        if schd_loss_update:\n",
        "            scheduler.step(loss_sum/sample_num)\n",
        "        else:\n",
        "            scheduler.step()"
      ],
      "metadata": {
        "id": "PH64OEDxwM3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "     # for training only, need nightly build pytorch\n",
        "\n",
        "    seed_everything(CFG['seed'])\n",
        "    \n",
        "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
        "    \n",
        "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
        "        # we'll train fold 0 first\n",
        "        if fold > 0:\n",
        "            break \n",
        "\n",
        "        print('Training with {} started'.format(fold))\n",
        "\n",
        "        print(len(trn_idx), len(val_idx))\n",
        "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root='/content/train_images/')\n",
        "\n",
        "        device = torch.device(CFG['device'])\n",
        "        \n",
        "        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n",
        "        scaler = GradScaler()   \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
        "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
        "        #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=25, \n",
        "        #                                                max_lr=CFG['lr'], epochs=CFG['epochs'], steps_per_epoch=len(train_loader))\n",
        "        \n",
        "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "        \n",
        "        for epoch in range(CFG['epochs']):\n",
        "            train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
        "\n",
        "            with torch.no_grad():  #파라미터 업데이트 현상 방지\n",
        "                valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
        "\n",
        "            torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n",
        "            \n",
        "        #torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n",
        "        del model, optimizer, train_loader, val_loader, scaler, scheduler\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2WTyOrkwNdr",
        "outputId": "eb8817b6-d58d-4138-d29e-719496afa318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 0 started\n",
            "17117 4280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
            "  FutureWarning,\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b4_ns-d6313a46.pth\n",
            "epoch 0 loss: 0.8033:  86%|████████▋ | 924/1070 [12:12<01:53,  1.28it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jrKK_6S2UhYc"
      }
    }
  ]
}